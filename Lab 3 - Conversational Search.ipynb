{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d41ec6",
   "metadata": {},
   "source": [
    "# Conversational Search with GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd860b-5997-45a6-869b-06793935c75d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "In this lab, we leverage LangChain framework to implement Retrieval Augmented Generation(RAG) solution. RAG retrieves data from outside the language model (non-parametric) and augments the prompts by adding the relevant retrieved data in context. RAG models were introduced by Lewis et al. in 2020 as a model where parametric memory is a pre-trained `seq2seq` model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n",
    "\n",
    "In RAG, external data can be sourced from various data sources, such as document repositories, databases, or APIs. The first step is to convert the documents and the user query into a format that enables comparison and allows for performing relevancy search. To achieve comparability for relevancy search, a document collection (knowledge library) and the user-submitted query are transformed into numerical representations using embedding language models. These embeddings are essentially numerical representations of concepts in text.\n",
    "\n",
    "Next, based on the embedding of the user query, relevant text is identified in the document collection through similarity search in the embedding space. The prompt provided by the user is then combined with the searched relevant text and added to the context. This updated prompt, which includes relevant external data along with the original prompt, is sent to the LLM (Language Model) for processing. As a result, the model output becomes relevant and accurate due to the context containing the relevant external data.\n",
    "\n",
    "For more informaiton about LangChain RAG, please refer: https://python.langchain.com/docs/use_cases/question_answering/\n",
    "\n",
    "---\n",
    "\n",
    "The lab includes the following steps:\n",
    "1. [Step 1: Initialize](#Step-1:-Initialize)\n",
    "2. [Step 2: Verify deployed endpoint for embedding and content generation model](#Step-2:-Verify-deployed-endpoint-for-embedding-and-content-generation-model)\n",
    "3. [Step 3: Test LLM without context information](#Step-3:-Test-LLM-without-context-information)\n",
    "    - [3.1 Test SageMaker](#3.1-Test-SageMaker)\n",
    "    - [3.2 Test Bedrock](#3.2-Test-Bedrock)\n",
    "4. [Step 4: Test LLM with context information](#Step-4:-Test-LLM-with-context-information)\n",
    "    - [4.1 Test SageMaker with Context](#4.1-Test-SageMaker-with-context)\n",
    "5. [Step 5: Select SageMaker or Bedrock used for embedding and content generation](#Step-5:-Select-SageMaker-or-Bedrock-used-for-embedding-and-content-generation)\n",
    "6. [Step 6: Load documents with LangChain document loader and store vector into OpenSearch](#Step-6:-Load-documents-with-LangChain-document-loader-and-store-vector-into-OpenSearch)\n",
    "7. [Step 7: Retrieval Augmented Generation](#Step-7:-Retrieval-Augmented-Generation)\n",
    "8. [Step 8: Conversational search by memorizing the history](#Step-8:-Conversational-search-by-memorizing-the-history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03874004-bd68-4fac-a05a-3834986a8e89",
   "metadata": {},
   "source": [
    "## Step 1: Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34490d00-970a-49ac-97f5-1999ff4ca03f",
   "metadata": {},
   "source": [
    "Install required library such as [OpenSearch client library](https://opensearch.org/docs/1.0/clients/python/), [LangChain](https://python.langchain.com/docs/get_started), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184986",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker==2.186.0 \n",
    "%pip install opensearch-py==2.3.1\n",
    "%pip install wikipedia unstructured transformers==4.33.2\n",
    "%pip install langchain==0.0.308 #0.0.293\n",
    "%pip install boto3==1.28.59\n",
    "%pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e422af0-3961-4e99-9f90-f46d3cf78f0b",
   "metadata": {},
   "source": [
    "Initialize SageMaker, and Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d0256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "aws_account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad970f25-32cf-4bd8-bfa4-f13e361a5b20",
   "metadata": {},
   "source": [
    "Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635d4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "region = aws_region\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "kms = boto3.client('secretsmanager')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926ca92-3373-47a7-973e-3136ebfa2370",
   "metadata": {},
   "source": [
    "## Step 2: Verify deployed endpoint for embedding and content generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2378ff-120e-4255-89f1-ea66949df044",
   "metadata": {},
   "source": [
    "As mentioned in the architecture diagram, we use two LLM in this lab:\n",
    "- Embedding Model: Convert text into vector\n",
    "- Text Generation LLM: Generate content \n",
    "\n",
    "---\n",
    "\n",
    "We will verify the two endpoins are ready before running the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49640d",
   "metadata": {},
   "source": [
    "### Get endpoint for embedding\n",
    "\n",
    "---\n",
    "This is SageMaker Endpoint with GPT-J 6B parameters model to convert text into vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5962e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_endpoint_name=outputs['EmbeddingEndpointName']\n",
    "print(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980c841-860a-46af-a3e2-2bf612c0134d",
   "metadata": {},
   "source": [
    "Endpoint should be in `InService` status to be able to serve requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef415501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "\n",
    "while describe_embedding_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_embedding_endpoint_response = sm_client.describe_endpoint(EndpointName=embedding_endpoint_name)\n",
    "print(describe_embedding_endpoint_response[\"EndpointStatus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b48027",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get endpoint for content generation\n",
    "\n",
    "We use Falcon large language model to generate text. Our [Falcon model](https://huggingface.co/tiiuae/falcon-7b) has 7 billion parameters. \n",
    "It is a smallest Falcon model available and provides a good balance between accuracy and hardware costs to run the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecca035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_endpoint_name=outputs['LLMEndpointName']\n",
    "print(llm_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2214c-3487-45a8-babf-d4bf3b3ff021",
   "metadata": {},
   "source": [
    "Verify embedding endpoint is ready (It should show the status as `InService`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d029d7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\", aws_region)\n",
    "\n",
    "describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "\n",
    "while describe_llm_endpoint_response[\"EndpointStatus\"] == 'Creating':\n",
    "    time.sleep(15)\n",
    "    print('.', end='')\n",
    "    describe_llm_endpoint_response = sm_client.describe_endpoint(EndpointName=llm_endpoint_name)\n",
    "print(describe_embedding_endpoint_response[\"EndpointStatus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b747507b",
   "metadata": {},
   "source": [
    "### Test embedding endpoint\n",
    "\n",
    "We use LangChain library to connect to the model deployed using SageMaker Endpoints.\n",
    "\n",
    "To handle the specific input and output format of the model we define `TestContentHandler` class with `transform_input` and `transform_output` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96eb56-b15e-4c46-9412-5d59ad09ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "\n",
    "\n",
    "class TestContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        if len(embeddings) == 1:\n",
    "            return [embeddings[0]]\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "test_content_handler = TestContentHandler()\n",
    "\n",
    "test_embeddings = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embedding_endpoint_name,\n",
    "    region_name=aws_region,\n",
    "    content_handler=test_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fb84f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# return first five elements of the embdedding\n",
    "print(test_embeddings.embed_documents([\"Hello World\"])[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa760ac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test LLM endpoint\n",
    "Alternatively we can also use AWS Boto3 library to query SageMaker endpoint as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86de9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "#method used to parse the inference model's response. we pass it as part of the model's config\n",
    "def parse_response_model(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0c128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"Hello OpenSearch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2146a8e",
   "metadata": {},
   "source": [
    "Quality, diveristy and creativity of the generated text can be controlled using the following parameters. \n",
    "\n",
    "**max_new_tokens**: The maximum number of new tokens to generate in the output text.\n",
    "\n",
    "**num_return_sequences**: The number of independent sequences to generate for each prompt.\n",
    "\n",
    "**top_k**: The number of highest probability tokens to consider for the next word generation.\n",
    "\n",
    "**top_p**: The cumulative probability threshold for generating the next word. It ensures diversity in the generated text by considering a broader range of tokens.\n",
    "\n",
    "**do_sample**: A boolean value indicating whether to use sampling for the next word generation or not. If set to False, the model will use greedy decoding, which means it will always choose the token with the highest probability. If set to True, the model will use sampling, which introduces randomness in the token selection process.\n",
    "\n",
    "**return_full_text**: A boolean value indicating whether to return the full generated text or just the newly generated tokens. If set to True it will return the input text provided to the model as well as the generated output.\n",
    "\n",
    "**temperature**: A parameter that controls the randomness of the next word generation. A higher temperature value (e.g., 1.0) will result in more random output, while a lower value (e.g., 0.1) will result in more deterministic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940794c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": question,\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.5,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": True,\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de27ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name\n",
    ")\n",
    "\n",
    "generated_texts = parse_response_model(query_response)\n",
    "\n",
    "print(f\"The generated output is: {generated_texts[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c4d59-f74f-4636-9e81-754a49bb2fef",
   "metadata": {},
   "source": [
    "## Step 3: Test LLM without context information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f1457",
   "metadata": {},
   "source": [
    "### 3.1 Test SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274d067-e103-4506-850c-1a8fae1d1132",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To better illustrate why we need retrieval-augmented generation (RAG) based approach to solve the question and anwering problem. LLM can generate answer without context information, however it might be incorrect or nonsensical. This behaviour is often referred to as LLM hallucination. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c089f940",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import DynamoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        #print(\"Prompt Input:\\n\" + input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(\"LLM generated text:\\n\" + response_json[0][\"generated_text\"])\n",
    "        return response_json[0][\"generated_text\"]\n",
    "    \n",
    "\n",
    "content_handler = ContentHandler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4247d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.9\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb236e6-e242-4d1a-bce3-e2f00409035a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_hallucination=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae6fc8-bb14-449e-8eef-0570a6d81093",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's directly ask the model a question and see how they respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to determine shard and data node counts for OpenSearch?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa531974-2eae-4d68-92c0-0a474a80a21d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "generated_result = llm_hallucination(question)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fa6466-9a91-433f-bb70-13daf78df4e2",
   "metadata": {},
   "source": [
    "Ask the same question again to compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16dcaa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "generated_result = llm_hallucination(question)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c80895",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The above examples are generated based on LLM training data, and they look plausible but factually incorrect.\n",
    "\n",
    "Moreover, every time you send the same question the generated output can be vastly different which might be the okay for the creative task but for fact-checking tasks is defintiely the disadvantage.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31632621",
   "metadata": {},
   "source": [
    "### 3.2 Test Bedrock\n",
    "\n",
    "Before you can start working with Bedrock you need to accept EULA agreement and enable the models access. Make sure you have completed the **Bedrock Console Configuration** lab first. \n",
    "<details>\n",
    "  <summary>Click <u>here</u> to see the instructions.</summary>\n",
    "\n",
    "  ![retriever](../image/module8/enable-models-bedrock.gif)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_bedrock_available=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "bedrock_region=\"us-west-2\"\n",
    "\n",
    "#boto3_bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=f\"https://bedrock-runtime.{bedrock_region}.amazonaws.com\")\n",
    "boto3_bedrock = boto3.client(service_name=\"bedrock-runtime\", config=Config(region_name=bedrock_region))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da798856",
   "metadata": {},
   "source": [
    "Here we use Titan text generation model. Same question and see if there is any hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffeedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "bedrock_llm_hallucination = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "bedrock_llm_hallucination.model_kwargs = {\"max_tokens_to_sample\":1204,\"temperature\":0.9,\"top_k\":250,\"top_p\":1,\"stop_sequences\":[\"\\\\n\\\\nHuman:\"]}\n",
    "\n",
    "if is_bedrock_available:\n",
    "    bedrock_result = bedrock_llm_hallucination(question)\n",
    "    print(bedrock_result)\n",
    "else:\n",
    "    print(\"Bedrock is unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a9c1e",
   "metadata": {},
   "source": [
    "## Step 4: Test LLM with context information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293d02bf",
   "metadata": {},
   "source": [
    "### 4.1 Test SageMaker with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e378b",
   "metadata": {},
   "source": [
    "To avoid LLM hallucination problem, we can provide context to LLM. LLM can generate content with the information in the context and the answer will be more relevant to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ec5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_context_params = {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 100,\n",
    "        \"top_p\": 0.95,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.01\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069f2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_context=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=with_context_params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede9aa41",
   "metadata": {},
   "source": [
    "Provide some context and ask the same question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ee2149",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_and_question=\"\"\"\n",
    "Answer the question with with the content in the Context.\n",
    "Context: \n",
    "By default in OpenSearch Service, each index is divided into five primary shards and one replica (total of 10 shards). This behavior differs from open source OpenSearch, which defaults to one primary and one replica shard. Because you can't easily change the number of primary shards for an existing index, you should decide about shard count before indexing your first document.\n",
    "The overall goal of choosing a number of shards is to distribute an index evenly across all data nodes in the cluster. However, these shards shouldn't be too large or too numerous. A general guideline is to try to keep shard size between 10–30 GiB for workloads where search latency is a key performance objective, and 30–50 GiB for write-heavy workloads such as log analytics.\n",
    "Large shards can make it difficult for OpenSearch to recover from failure, but because each shard uses some amount of CPU and memory, having too many small shards can cause performance issues and out of memory errors. In other words, shards should be small enough that the underlying OpenSearch Service instance can handle them, but not so small that they place needless strain on the hardware.\n",
    "For example, suppose you have 66 GiB of data. You don't expect that number to increase over time, and you want to keep your shards around 30 GiB each. Your number of shards therefore should be approximately 66 * 1.1 / 30 = 3. \n",
    "\n",
    "Question:How to determine shard and data node counts for OpenSearch?\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "generated_result = llm_with_context(context_and_question)\n",
    "print(generated_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac46c1",
   "metadata": {},
   "source": [
    "---\n",
    "By comparing the output without context and with context, you will see see the difference. By providing context, LLM generated answer will be more related to domain query. However we can not provide all context manually to LLM. How to provide context to all query is what RAG can help here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e258274",
   "metadata": {},
   "source": [
    "## Step 5: Select SageMaker or Bedrock used for embedding and content generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f219795",
   "metadata": {},
   "source": [
    "Select one of the LLM used in the following steps. You can use SageMaker or Bedrock in this lab. First you can select SageMaker and complete the following steps. Then go back here to select Bedrock again.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "\n",
    "llm_selection = [\n",
    "    \"SageMaker\",\n",
    "    \"Bedrock\",\n",
    "]\n",
    "\n",
    "llm_dropdown = Dropdown(\n",
    "    options=llm_selection,\n",
    "    value=\"SageMaker\",\n",
    "    description=\"Select a LLM\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(llm_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd2206",
   "metadata": {},
   "source": [
    "### Note\n",
    "If Bedrock is unavailable, we have to use SageMaker as backup plan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276082d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_category = llm_dropdown.value\n",
    "\n",
    "if not is_bedrock_available:\n",
    "    llm_category = \"SageMaker\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc4142",
   "metadata": {},
   "source": [
    "Print the LLM you will use in the following lab\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You selected {0} as LLM\".format(llm_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdeaa9",
   "metadata": {},
   "source": [
    "## Step 6: Load documents with LangChain document loader and store vector into OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128529b-29fc-4e00-9faa-e5de2de70023",
   "metadata": {},
   "source": [
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "---\n",
    "\n",
    "The following is data flow diagram of loading documents and store vector into OpenSearch.\n",
    "\n",
    "![retriever](../image/module8/document-loader.png)\n",
    "\n",
    "\n",
    "Document loaders expose a \"load\" method for loading data as documents from a configured source. Here, we use `UnstructuredURLLoader` to load OpenSearch best practice web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0369d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.document_loaders import SeleniumURLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "\n",
    "urls = [\"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/bp.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/sizing-domains.html\", \n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/petabyte-scale.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains-dedicatedmasternodes.html\",\n",
    "        \"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/cloudwatch-alarms.html\"]\n",
    "url_loader = UnstructuredURLLoader(urls=urls)\n",
    "#url_loader = SeleniumURLLoader(urls=urls)\n",
    "url_texts = url_loader.load_and_split(text_splitter=text_splitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4d4df-fbb9-448a-9f43-f33fe7df9dd8",
   "metadata": {},
   "source": [
    "Show one example document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e53165-93fd-4434-a653-40ba1e98c67d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_splits = url_texts\n",
    "print(all_splits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57321a3a-db5c-4589-b6a4-6a007154d067",
   "metadata": {},
   "source": [
    "Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with Amazon Opensearch Service domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede13e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a03874-5a51-456e-b36c-e31771d73ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LangChain embedding endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671fdfd-0632-49b8-b7f7-05493baa3ece",
   "metadata": {},
   "source": [
    "To build a simiplied QA application with LangChain, we need to wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. That requires a overwrite methods of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding mdoel.\n",
    "\n",
    "---\n",
    "\n",
    "Embedding language model is GPT-J, and the endpoint name is `embedding_endpoint_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dd4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Callable\n",
    "import json\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.schema import Document\n",
    "\n",
    "class BulkSagemakerEndpointEmbeddings(SagemakerEndpointEmbeddings):\n",
    "        def embed_documents(\n",
    "            self, texts: List[str], chunk_size: int = 5\n",
    "        ) -> List[List[float]]:\n",
    "            results = []\n",
    "            _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "            for i in range(0, len(texts), _chunk_size):\n",
    "                response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "                results.extend(response)\n",
    "            return results\n",
    "        \n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "            input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "            return input_str.encode('utf-8') \n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            embeddings = response_json[\"embedding\"]\n",
    "            if len(embeddings) == 1:\n",
    "                return [embeddings[0]]\n",
    "            return embeddings\n",
    "\n",
    "print(embedding_endpoint_name)\n",
    "sagemaker_embeddings = BulkSagemakerEndpointEmbeddings( \n",
    "            endpoint_name=embedding_endpoint_name,\n",
    "            region_name=aws_region, \n",
    "            content_handler=EmbeddingContentHandler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56027a",
   "metadata": {},
   "source": [
    "### Bedrock embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b84fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id='amazon.titan-embed-text-v1',client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57297dfc-3816-4039-8a61-35e0ff1965cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OpenSearch vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bf2a9",
   "metadata": {},
   "source": [
    "Provide embedding service based on selection between SageMaker and Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49401f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        embeddings = sagemaker_embeddings\n",
    "    case \"Bedrock\":\n",
    "        embeddings = bedrock_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b01f2f",
   "metadata": {},
   "source": [
    "Show the the embedding service to be used, SageMaker or Bedrock selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe272ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c04e0",
   "metadata": {},
   "source": [
    "Initialize OpenSearch index name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index_name = 'opensearch_kb_vector'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894ffec",
   "metadata": {},
   "source": [
    "Uncomment the following code if this is the second time to run this lab to remove existing index in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a499a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#aos_client.indices.delete(index=embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0aeb8-d5e5-4f1c-b9c4-432afdbf93b4",
   "metadata": {},
   "source": [
    "Use `OpenSearchVectorSearch` in LangChain to ingest vector into OpenSearch. You can specify more parameters to create kNN index with specified properties. Some parameters like:\n",
    "engine: “nmslib”, “faiss”, “lucene”; default: “nmslib”\n",
    "\n",
    "space_type: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; default: “l2”\n",
    "\n",
    "ef_search: Size of the dynamic list used during k-NN searches. Higher values lead to more accurate but slower searches; default: 512\n",
    "\n",
    "ef_construction: Size of the dynamic list used during k-NN graph creation. Higher values lead to more accurate graph but slower indexing speed; default: 512\n",
    "\n",
    "m: Number of bidirectional links created for each new element. Large impact on memory consumption. Between 2 and 100; default: 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d425b-0aa0-4e40-9be8-db082b3c5e0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "os_domain_ep = 'https://'+aos_host\n",
    "\n",
    "if len(all_splits) > 500:\n",
    "    for i in range(0, len(all_splits), 500):\n",
    "        start = i\n",
    "        end = i+500\n",
    "        if end > len(all_splits):\n",
    "            end = len(all_splits)-1\n",
    "        docs = all_splits[start:end]\n",
    "        OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "        print(f\"ingest documents from {start} to {end}\", start, end)\n",
    "else:\n",
    "    OpenSearchVectorSearch.from_documents(\n",
    "            index_name = embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth\n",
    "        )\n",
    "    print(f\" completed documents ingestion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0d643",
   "metadata": {},
   "source": [
    "The the OpenSearch index detailed information. Pay attention to `dimension` field value when you choose different LLM. For SageMaker, we use GPT-J enmbedding model whose dimension is 4096. For Bedrock, we use Titan Embedding modele whose dimension is 1536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369f552-2b2c-49fe-955b-fd814183b720",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94d29f",
   "metadata": {},
   "source": [
    "Initialize OpenSearch index name whose settings are customized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4853597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_embedding_index_name = 'customized_opensearch_kb_vector'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea3dd34",
   "metadata": {},
   "source": [
    "Uncomment the following code if this is the second time to run this lab to remove existing index in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c29f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aos_client.indices.delete(index=customized_embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0d9dd-0350-41ac-a844-2d64ecd11dce",
   "metadata": {},
   "source": [
    "When you use LangChain `OpenSearchVectorSearch` to store embedding with OpenSearch kNN index, you can specify parameters to choose different Approximate Near Neighbour(ANN) algorithms. For more information, please refer OpenSearch kNN documentaion: https://opensearch.org/docs/latest/search-plugins/knn/knn-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610f4d3-1aa7-4a03-a668-90ebd4874d9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "OpenSearchVectorSearch.from_documents(\n",
    "            index_name = customized_embedding_index_name,\n",
    "            documents=all_splits,\n",
    "            embedding=embeddings,\n",
    "            opensearch_url=os_domain_ep,\n",
    "            http_auth=auth,\n",
    "            engine=\"faiss\",\n",
    "            space_type=\"innerproduct\",\n",
    "            ef_construction=256,\n",
    "            m=48,\n",
    "        )\n",
    "print(f\"ingest documents into customized knn index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673dbddf",
   "metadata": {},
   "source": [
    "Get OpenSearch index detailed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa8d63-6a13-42dd-9123-8d6ef19218a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=customized_embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ecb4d-45ab-4cb7-8fa4-64cc7ef322a7",
   "metadata": {},
   "source": [
    "We can use `OpenSearchVectorSearch` for vector store or we can extend the class to define new fuction to calculate documents relevance score if you want to use relevance score to filter document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110d127-b60b-45e1-a178-37fd175537ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimiliarOpenSearchVectorSearch(OpenSearchVectorSearch):\n",
    "    \n",
    "    def relevance_score(self, distance: float) -> float:\n",
    "        return distance\n",
    "    \n",
    "    def _select_relevance_score_fn(self) -> Callable[[float], float]:\n",
    "        return self.relevance_score\n",
    "    \n",
    "\n",
    "open_search_vector_store = SimiliarOpenSearchVectorSearch(\n",
    "                                    index_name=embedding_index_name,\n",
    "                                    embedding_function=embeddings,\n",
    "                                    opensearch_url=os_domain_ep,\n",
    "                                    http_auth=auth\n",
    "                                    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92332ed7-546f-493c-8f42-705275bf70e7",
   "metadata": {},
   "source": [
    "Show the documents which are similiar with question \"How to determine shard and data node counts for OpenSearch?\". Be default, 4 documents are returned. You can specify \"k\" parameter. See the [doc](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html#langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.similarity_search_with_score) for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644b724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_ = open_search_vector_store.similarity_search_with_score(question, k=5)\n",
    "\n",
    "print(\"found document number:\" + str(len(docs_)))\n",
    "\n",
    "print(\"opensearch results:\\n\")\n",
    "for doc in docs_:\n",
    "    print(doc)\n",
    "    print(\"\\n-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0e71e-bf17-4f6e-aaff-be56a874c212",
   "metadata": {},
   "source": [
    "## Step 7: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db375107-9d31-4e35-acda-a5d20941ca7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To mitigate LLM hallucination, we can provide some context to LLM and let LLM generated answer with the context. The following diagram show RAG data flow:\n",
    "\n",
    "![rag](../image/module8/rag.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07e9fa",
   "metadata": {},
   "source": [
    "In this lab,  we use OpenSearch vector store as retriever to get similiar documents with query. We can also specify similarity scrore threshhold to return high relevant documents. Use \"k\" to limit how many documents to be returned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55211dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sagemaker_retriever = open_search_vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        'k': 5,\n",
    "        'score_threshold': 0.62\n",
    "    }\n",
    ")\n",
    "\n",
    "bedrock_retriever = open_search_vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        'k': 5,\n",
    "        'score_threshold': 0.005\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        retriever = sagemaker_retriever\n",
    "    case \"Bedrock\":\n",
    "        retriever = bedrock_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2657b1",
   "metadata": {},
   "source": [
    "Define SageMaker LLM endpoint\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29542ec7-0296-4f3e-b89d-fe9064665960",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_params = {\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 200,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0001\n",
    "        }\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e45446",
   "metadata": {},
   "source": [
    "Define Bedrock content generation \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_params = {\n",
    "    \"max_tokens_to_sample\":2048,\n",
    "    \"temperature\":0.0001,\n",
    "    \"top_k\":250,\n",
    "    \"top_p\":1,\n",
    "    \"stop_sequences\":[\"\\\\n\\\\nHuman:\"]\n",
    "}\n",
    "\n",
    "bedrock_titan_llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "bedrock_titan_llm.model_kwargs = bedrock_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7422be1",
   "metadata": {},
   "source": [
    "### Select content generation LLM, SageMaker or Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22732f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        llm = sagemaker_llm\n",
    "    case \"Bedrock\":\n",
    "        llm = bedrock_titan_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92a6d1",
   "metadata": {},
   "source": [
    "Show the the LLM for content generation to be used, SageMaker or Bedrock selected above\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3bba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9bf32",
   "metadata": {},
   "source": [
    "Define `RetrievalQA` Chain with SageMaker or Bedrock LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a1e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\" #stuff, refine, map_reduce, and map_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869cfe8-963f-4053-aa00-22f7ff76a132",
   "metadata": {},
   "source": [
    "Use RAG to generate answer to the same question before. Compare the content generated with RAG and LLM without context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feaca4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "\n",
    "#langchain.debug=False\n",
    "result = qa({\"query\": question})\n",
    "\n",
    "print(\"result:\" + result[\"result\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774aac71-7220-4205-bf2d-1ed253dd5b1e",
   "metadata": {},
   "source": [
    "### Use customized prompt for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479f506-bae2-4573-8126-7b2fa6b42182",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "Though the content generated by RAG is much better than the answer without any content. However some of the generated bullets are duplicate. You can improve the generated content by optimizing the prompter to LLM. Following is example of using customized prompt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959e688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template2 = \"\"\"Answer the question as truthfully as possible by using the provided informaiton in >>CONTEXT<<. If the answer is not contained within the >>CONTEXT<<, respond with \"I can't answer that\".\n",
    "\n",
    ">>CONTEXT<<:\n",
    "{context}\n",
    "\n",
    ">>QUESTION<<:\n",
    "{question}\n",
    "\n",
    ">>Answer<<:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_template2 = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=template2\n",
    ")\n",
    "\n",
    "qa_with_prompt = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template2}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de5dec",
   "metadata": {},
   "source": [
    "Run the code qa chain with question. You can set `langchain.debug = True` if you want to see the debug informaiton.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8517d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "#langchain.debug = True\n",
    "langchain.debug = False\n",
    "\n",
    "print(\"Question is:\" + question)\n",
    "result = qa_with_prompt({\"query\": question})\n",
    "\n",
    "print(\"\\n### Generated result:\" + result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5710a4f-5652-4a2d-a126-0ac8e10aceef",
   "metadata": {},
   "source": [
    "### Return source documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1054077-8a3f-40e1-a5e8-5ef65cbb11c1",
   "metadata": {},
   "source": [
    "You can also return the source documents to help you locate original knowledge base document which are related with the question.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba727f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qa_with_source = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131847b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "result = qa_with_source({\"query\": question})\n",
    "\n",
    "print(\"result:\" + result[\"result\"])\n",
    "print(\"\\n\\n===========================\")\n",
    "print(\"\\nsource documents:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc)\n",
    "    print(\"---------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c88750-e300-45f9-988f-3e238e72f84f",
   "metadata": {},
   "source": [
    "## Step 8: Conversational search by memorizing the history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d64dc6-df81-4df2-806a-14cebec9b6ce",
   "metadata": {},
   "source": [
    "### LangChain Memory with Amazon DynamoDB as data store\n",
    "\n",
    "In the above example, you can ask any questions to the system. However there is no relation among the questions. In a typical search system, you may want to implement conversational search. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain. In this lab, we use [Amazon DynamoDB](https://aws.amazon.com/dynamodb/) as data store of history message.\n",
    "\n",
    "---\n",
    "The data flow of conversational search with memory is as following:\n",
    "\n",
    "![rag](../image/module8/rag-with-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0302347",
   "metadata": {},
   "source": [
    "Check if the dynamodb table exits and optionally create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78531186",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamo = boto3.client('dynamodb')\n",
    "\n",
    "history_table_name = 'conversation-history-memory'\n",
    "\n",
    "try:\n",
    "    response = dynamo.describe_table(TableName=history_table_name)\n",
    "    print(\"The table \"+history_table_name+\" exists\")\n",
    "except dynamo.exceptions.ResourceNotFoundException:\n",
    "    print(\"The table \"+history_table_name+\" does not exist\")\n",
    "    \n",
    "    dynamo.create_table(\n",
    "    TableName=history_table_name,\n",
    "    AttributeDefinitions=[\n",
    "        {\n",
    "            'AttributeName': 'SessionId',\n",
    "            'AttributeType': 'S',\n",
    "        }\n",
    "    ],\n",
    "    KeySchema=[\n",
    "        {\n",
    "            'AttributeName': 'SessionId',\n",
    "            'KeyType': 'HASH',\n",
    "        }\n",
    "    ],\n",
    "    ProvisionedThroughput={\n",
    "        'ReadCapacityUnits': 5,\n",
    "        'WriteCapacityUnits': 5,\n",
    "    }\n",
    "    )\n",
    "\n",
    "    response = dynamo.describe_table(TableName=history_table_name) \n",
    "    \n",
    "    while response[\"Table\"][\"TableStatus\"] == 'CREATING':\n",
    "        time.sleep(1)\n",
    "        print('.', end='')\n",
    "        response = dynamo.describe_table(TableName=history_table_name) \n",
    "\n",
    "    print(\"\\ndynamo DB Table, '\"+response['Table']['TableName']+\"' is created\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824f779-5233-4047-a26a-38e035354445",
   "metadata": {},
   "source": [
    "---\n",
    "Here we create new session and use DynamoDB as backend to store history conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea68c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddb_table_name = \"conversation-history-memory\"\n",
    "session_id = str(uuid4())\n",
    "chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "messages = chat_memory.messages\n",
    "\n",
    "# Maintains immutable sessions\n",
    "# If previous session was present, create\n",
    "# a new session and copy messages, and \n",
    "# generate a new session_id \n",
    "if messages:\n",
    "    session_id = str(uuid4())\n",
    "    chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "    # This is a workaround at the moment. Ideally, this should\n",
    "    # be added to the DynamoDBChatMessageHistory class\n",
    "    try:\n",
    "        messages = messages_to_dict(messages)\n",
    "        chat_memory.table.put_item(\n",
    "            Item={\"SessionId\": session_id, \"History\": messages}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "memory = ConversationBufferMemory(chat_memory=chat_memory, memory_key=\"chat_history\", return_messages=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbb51f-2460-4a8e-a482-08be21b02e26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Create `ConversationalRetrievalChain` to combines the chat history and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question-answering chain to return a response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb0a3e-40a1-4927-8833-5f185410eef8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "params = {\n",
    "        \"max_length\": 2048,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 200,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0001\n",
    "        }\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")\n",
    "\n",
    "condense_template = \"\"\"system: generate one standalone question.\n",
    "Given the following conversation between <chat-history> and </chat-history> \n",
    "and follow up question between <follow-up-question> and </follow-up-question>, \n",
    "rephrase the follow up question to be a standalone question in its original language. \n",
    "The standalone question will only contains one sentence and it must end with '?'\n",
    "\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-question>\n",
    "{question}\n",
    "</follow-up-question>\n",
    "\n",
    "standalone question:\n",
    "\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        llm = sagemaker_llm\n",
    "    case \"Bedrock\":\n",
    "        llm = bedrock_titan_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bc823",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template2},\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd000156-1110-4429-b8b9-1a34f1e5ba61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For the first question, there is no history. It is just standard RAG process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc1671-79f2-4056-8248-6eff1e3c16fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = qa_with_memory(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02d8f4-4a23-412f-a76d-41e75037ad9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"result:\" + str(result))\n",
    "print(\"\\nAnswer:\\n\" + str(result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cdd8c-a04d-41fe-99e6-33b531015260",
   "metadata": {},
   "source": [
    "### Second question\n",
    "Try to ask one more question, `ConversationalRetrievalChain` will use the first question, first question's answer and second question as prompt to LLM to generate new question. The prompt to LLM is like following:\n",
    "\n",
    "```python\n",
    "\n",
    "_template = \"\"\"system: generate one standalone question.\n",
    "Given the following conversation between <chat-history> and </chat-history> \n",
    "and follow up question between <follow-up-question> and </follow-up-question>, \n",
    "rephrase the follow up question to be a standalone question in its original language. \n",
    "The standalone question will only contains one sentence and it must end with '?'\n",
    "\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-question>\n",
    "{question}\n",
    "</follow-up-question>\n",
    "\n",
    "standalone question:\n",
    "```\n",
    "\n",
    "After get the new question from LLM, it will search relevant document from OpenSearch vector store and get relevant documents, then combine the new question and relevant documents as prompt to go through RAG process. The prompt to LLM is like following:\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"Answer the question as truthfully as possible by using the provided informaiton in >>CONTEXT<<. If the answer is not contained within the >>CONTEXT<<, respond with \"I can't answer that\".\n",
    "\n",
    ">>CONTEXT<<:\n",
    "{context}\n",
    "\n",
    ">>QUESTION<<:\n",
    "{question}\n",
    "\n",
    ">>Answer<<:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "In summary, `ConversationalRetrievalChain` will call LLM twice:\n",
    "1. Use history question, history answer and latest question as prompt to generate new question\n",
    "2. Use new question generated in the first step, query relevant documents. Combine relevant documents and new question as prompt to LLM to generate answer.\n",
    "\n",
    "You can also see the verbose message like following:\n",
    "\n",
    "---\n",
    "\n",
    "### First call to LLM:\n",
    "\n",
    "![generate new question](../image/module8/conversation-new-question.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Second call to LLM:\n",
    "\n",
    "![generate final answer](../image/module8/conversation-final-answer.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e63fc-6c54-479b-b7c5-8310a16896a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_following_question = 'if my data growth is very fast'\n",
    "second_result = qa_with_memory(second_following_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8099a-a3be-4996-9852-97a6282d85e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"second answer:\" + str(second_result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07d06d-eabf-425a-a9ed-808abf76a271",
   "metadata": {},
   "source": [
    "### Return source document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cc063-ede9-4bed-b780-912a4cbc583c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We can also include source documents so that we can know where the content information are from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3d864-222a-41f7-a88f-d00754cacda1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "session_id = str(uuid4())\n",
    "chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "memory_for_source = ConversationBufferMemory(chat_memory=chat_memory,memory_key=\"chat_history\")\n",
    "\n",
    "qa_with_memory_and_source = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template2},\n",
    "    return_source_documents=True, \n",
    "    verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89fbf6-b500-46ef-af80-006ab3c610e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "result = qa_with_memory_and_source({\"question\": question,\"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da474e5-a8d9-4739-a135-5a686146dc7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nAnswer:\\n\" + str(result[\"answer\"]))\n",
    "print(\"\\nSource Documents:\\n\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(str(doc))\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6aa7c-6e9f-41fa-9a7f-fc29b288a1ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_history = [(question, result[\"answer\"])]\n",
    "second_query = second_following_question\n",
    "second_result = qa_with_memory_and_source({\"question\": second_query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8965962-cd1d-481b-bed2-e3f475252814",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\nAnswer:\\n\" + str(second_result[\"answer\"]))\n",
    "print(\"\\nSource Documents:\\n\")\n",
    "for doc in second_result[\"source_documents\"]:\n",
    "    print(str(doc))\n",
    "    print(\"--------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8368d5ed",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Step 9: Try another LLM \n",
    "\n",
    "---\n",
    "Go back to [Step 5: Select SageMaker or Bedrock used for embedding and content generation](#Step-5:-Select-SageMaker-or-Bedrock-used-for-embedding-and-content-generation) to try another LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
