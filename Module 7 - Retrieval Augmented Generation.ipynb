{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "We will use the semantic search to provide the best matching wine based on the review description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31703e3d",
   "metadata": {},
   "source": [
    "### 1. Check PyTorch Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As in the previous modules, let's import PyTorch and confirm that have have the latest version of PyTorch. The version should already be 1.13.1 or higher. If not, please run the lab in order to get everything set up."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install --upgrade pandas==1.3.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Retrieve notebook variables\n",
    "\n",
    "The line below will retrieve your shared variables from the previous notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%store -r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Install OpenSearch ML Python library"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install opensearch-py-ml\n",
    "!pip install accelerate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to restart the kernel by running below cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Import library\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Prepare data\n",
    "\n",
    "This lab combines semantic search with a generative model to present the retrieved data to the user in a conversational tone. This is a dataset of wine reviews. We'll sample this data set to recomend wines that resemble the user provided description.\n",
    "\n",
    "### Note\n",
    "You can download the dataset from various sources. One is Kaggle.\n",
    "https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "After downloading and copying here, unzip in the working directory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!unzip -o winemag-data-130k-v2.json.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('winemag-data-130k-v2.json')\n",
    "\n",
    "df.sample(3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# wm_list = df.to_dict('records')\n",
    "wm_list = df.sample(300,\n",
    "                   random_state=37).to_dict('records') # sample to keep lab quick\n",
    "\n",
    "wm_list[:5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "Note: if you're using a region other than us-east-1, please update the region in the code below.\n",
    "\n",
    "#### Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "\n",
    "region = 'us-east-1' \n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (\"master\",\"Semantic123!\")\n",
    "index_name = 'nlp_wmd'\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Configure OpenSearch domain to enable run Machine Learning code in data node"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = b'{\"transient\":{\"plugins.ml_commons.only_run_on_ml_node\": false}}'\n",
    "aos_client.cluster.put_settings(body=s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verify `plugins.ml_commons.only_run_on_ml_node` is set to false"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aos_client.cluster.get_settings(flat_settings=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. Download pre-trained BERT model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://github.com/opensearch-project/ml-commons/raw/2.x/ml-algorithms/src/test/resources/org/opensearch/ml/engine/algorithms/text_embedding/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip?raw=true', 'model/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verify model is downloaded successfully in the `model` folder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!ls -al model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9. Upload BERT model to OpenSearch domain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from opensearch_py_ml.ml_models import SentenceTransformerModel\n",
    "from opensearch_py_ml.ml_commons import MLCommonClient\n",
    "\n",
    "ml_client = MLCommonClient(aos_client)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "model_path = './model/all-MiniLM-L6-v2_torchscript_sentence-transformer.zip'\n",
    "model_config_path = './model/all-MiniLM-L6-v2_torchscript.json'\n",
    "\n",
    "\n",
    "model_id=ml_client.upload_model(model_path, model_config_path, isVerbose=True)\n",
    "\n",
    "print(\"model id:\" + model_id)\n",
    "\n",
    "ml_client.unload_model(model_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10. Load the model for inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "load_model_output = ml_client.load_model(model_id)\n",
    "\n",
    "print(load_model_output)\n",
    "task_id = load_model_output['task_id']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the task detailed information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_info = ml_client.get_task_info(task_id)\n",
    "\n",
    "print(task_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the model detailed information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_info = ml_client.get_model_info(model_id)\n",
    "\n",
    "print(model_info)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 11. Create pipeline to convert text into vector with BERT model\n",
    "We will use the just uploaded model to convert `qestion` field into vector(embedding) and stored into `question_vector` field."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pipeline={\n",
    "  \"description\": \"An example neural search pipeline\",\n",
    "  \"processors\" : [\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": model_id,\n",
    "        \"field_map\": {\n",
    "           \"description\": \"description_vector\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "pipeline_id = 'nlp_pipeline'\n",
    "aos_client.ingest.put_pipeline(id=pipeline_id,body=pipeline)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verify pipeline is created succefuflly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aos_client.ingest.get_pipeline(id=pipeline_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12. Create a index in Amazon Opensearch Service \n",
    "Whereas we previously created an index with 2 fields, this time we'll define the index with 3 fields: the first field ' question_vector' holds the vector representation of the question, the second is the \"question\" for raw sentence and the third field is \"answer\" for the raw answer data.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"default_pipeline\": pipeline_id,\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 384,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"faiss\"\n",
    "                },\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If for any reason you need to recreate your dataset, you can uncomment and execute the following to delete any previously created indexes. If this is the first time you're running this, you can skip this step."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# aos_client.indices.delete(index=\"nlp_pqa\") # drop the index from the previous lab\n",
    "\n",
    "# If this is the first time you're running this, you won't have this index to drop\n",
    "# aos_client.indices.delete(index=\"nlp_wmd\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aos_client.indices.create(index=\"nlp_wmd\",body=knn_index,ignore=400)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's verify the created index information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=\"nlp_wmd\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 13. Load the raw data into the Index\n",
    "Next, let's load the headset enhanced PQA data into the index we've just created. During ingest data, `question` field will also be converted to vector(embedding) by the `nlp_pipeline` we defined."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "for c in wm_list:\n",
    "    content=c['description']\n",
    "    description=c['description']\n",
    "    points=c[\"points\"]\n",
    "    variety=c[\"variety\"]\n",
    "    country=c[\"country\"]\n",
    "    designation=c[\"designation\"]\n",
    "    winery=c[\"winery\"]\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    aos_client.index(index='nlp_wmd',body={\n",
    "        \"content\": content,\n",
    "        \"points\": points,\n",
    "        \"variety\": variety,\n",
    "        \"country\": country,\n",
    "        \"description\": description,\n",
    "        \"designation\": designation,\n",
    "        \"winery\": winery,\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 300 hits in the index."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = aos_client.search(index=\"nlp_wmd\", body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 14. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def query_wines(desired_description, n=1):\n",
    "    osquery={\n",
    "      \"_source\": {\n",
    "            \"exclude\": [ \"description_vector\" ]\n",
    "        },\n",
    "      \"size\": 30,\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"description_vector\": {\n",
    "            \"query_text\": desired_description,\n",
    "            \"model_id\": model_id,\n",
    "            \"k\": 30\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=\"nlp_wmd\", \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"])\n",
    "\n",
    "    print(\"Got %d Hits:\" % res['hits']['total']['value'])\n",
    "    query_result=[]\n",
    "    for hit in res['hits']['hits']:\n",
    "        row=[\n",
    "                hit['_id'],\n",
    "                hit['_score'],\n",
    "                hit['_source']['description'],\n",
    "                hit['_source']['winery'],\n",
    "                hit['_source']['points'],\n",
    "                hit['_source']['designation'],\n",
    "                hit['_source']['country'],\n",
    "                hit['_source']['variety'],\n",
    "            ]\n",
    "        query_result.append(row)\n",
    "\n",
    "    query_result_df = pd.DataFrame(data=query_result,columns=[\n",
    "                                                            \"_id\",\n",
    "                                                            \"_score\",\n",
    "                                                            \"description\",\n",
    "                                                            \"winery\", \n",
    "                                                            \"points\", \n",
    "                                                            \"designation\",\n",
    "                                                            \"country\",     \n",
    "                                                            \"variety\",\n",
    "                                                         ])\n",
    "    \n",
    "    query_result_df.drop(['_id', '_score'], inplace=True, axis=1)\n",
    "    result = query_result_df.head(n).to_dict('records')\n",
    "    return result\n",
    "\n",
    "\n",
    "example_request = 'big and bold, jammy, blackberries'\n",
    "example_review = query_wines(example_request, 2)[0]\n",
    "\n",
    "example_review"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 15. Deploy the LLM\n",
    "\n",
    "This lab uses [AlexaTM 20B](https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/) model to create recomendations based on a given wine review. The next cell deploys a model endpoint into your environment that will be called by subsequent steps."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sagemaker, json\n",
    "from sagemaker import get_execution_role\n",
    "from datetime import datetime\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "aws_role = get_execution_role()\n",
    "\n",
    "# model_version = \"*\" fetches the latest version of the model\n",
    "model_id, model_version = \"pytorch-textgeneration1-alexa20b\", \"*\"\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-console-infer-{model_id}\")\n",
    "\n",
    "endpoint_config_name = \"config-\" + endpoint_name\n",
    "\n",
    "\n",
    "# GPU Instance Reqts: >50 GB of CPU RAM and >42 GB of GPU memory in total\n",
    "# Tested with ml.g4dn.12xlarge, ml.p3.8xlarge and ml.p3.16xlarge\n",
    "# instance_type = \"ml.g4dn.12xlarge\"\n",
    "instance_type = \"ml.p3.8xlarge\"\n",
    "\n",
    "# If using an EBS-backed instance, you must specify at least 256 GB of storage\n",
    "# If using an instance with local SSD storage, volume_size must be None\n",
    "if instance_type == \"ml.g4dn.12xlarge\":\n",
    "    volume_size = None\n",
    "elif instance_type in [\"ml.p3.8xlarge\", \"ml.p3.16xlarge\"]:\n",
    "    volume_size = 256\n",
    "else:\n",
    "    volume_size = None\n",
    "    print(\n",
    "        f\"Instance_type={instance_type} not tested. Setting volume_size = None.\"\n",
    "        \"If you run into out of space errors and your instance supports EBS storage,\"\n",
    "        \"please set volume_size = 256.\"\n",
    "    )\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base PyTorch container image.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri. This includes both pre-trained parameters, inference handling scripts and any dependencies.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "env = {\n",
    "    \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": str(7200),\n",
    "    \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
    "    \"SAGEMAKER_ENV\": \"1\",\n",
    "    \"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/code/\",\n",
    "    \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
    "    \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",  # without this, there will be one process per GPU\n",
    "    \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\",  # without this, each worker will have 1/num_gpus the RAM\n",
    "}\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    "    env=env,\n",
    ")\n",
    "\n",
    "print(\"☕ Spinning up the endpoint. This will take a little while ☕\")\n",
    "\n",
    "# deploy the Model.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=volume_size,  # Specify the size of the Amazon EBS volume.\n",
    "    model_data_download_timeout=3600,  # Specify the model download timeout in seconds.\n",
    "    container_startup_health_check_timeout=3600,  # Specify the health checkup timeout in seconds\n",
    ")\n",
    "\n",
    "# If you already deployed a model, comment the above model_predictory instantiation using model.deploy() and\n",
    "# add your endpoint name below\n",
    "# model_predictor = Predictor(endpoint_name=\"jumpstart-console-infer-pytorch-textgen-2023-04-11-01-23-42-572\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 15 Query the LLM with a test recomendation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def query(model_predictor, text, generate_kwargs=None, max_num_attempts=5):\n",
    "    \"\"\"Query the model predictor.\n",
    "\n",
    "    model_predictor: The deployed model pipeline.\n",
    "    text: a string or list of strings to input to the model pipeline.\n",
    "    generate_kwargs: A dictionary of generation arguments.\n",
    "    max_num_attempts: Maximum number of invokation request.\n",
    "\n",
    "    returns: A JSON of the model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    payload = {\"text_inputs\": text}\n",
    "    if generate_kwargs is not None:\n",
    "        payload.update(generate_kwargs)\n",
    "\n",
    "    encoded_inp = json.dumps(payload).encode(\"utf-8\")\n",
    "    for _ in range(max_num_attempts):\n",
    "        try:\n",
    "            query_response = model_predictor.predict(\n",
    "                encoded_inp,\n",
    "                {\"ContentType\": \"application/json\", \"Accept\": \"application/json\"},\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Invokation request unsuccessful. Retrying.\")\n",
    "            continue\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the list of generated texts.\"\"\"\n",
    "\n",
    "    return json.loads(query_response)[\"generated_texts\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 16. Create a prompt using the search results\n",
    "\n",
    "The AlexaTM 20B model performs well for single shot generation. The render_prompt function creates a single shot prompt using the requested description and a pre-made example response. You can read more about the AlexaTM 20B model on [Amazon Science](https://www.amazon.science/publications/alexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remember, we have an example user request wine description in the 'example_request' variable\n",
    "\n",
    "print(example_request)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def render_prompt(requested_description):\n",
    "    recomendation = query_wines(requested_description)[0]\n",
    "#     print(f\"Recomendation: {recomendation}\")\n",
    "    sample_recomendation = \"{{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}}\"\n",
    "    sample_response = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    \n",
    "    prompt = (\n",
    "        f\"[CLM] Context: A sommelier uses their vast knowledge of wine to make great recomendations people will enjoy. A recomendation always includes the winery, the country of origin, and a colorful description.\"\n",
    "        f\"Data: <br> Recomendation: <br>\"\n",
    "        f\"Data: {sample_recomendation} <br> Recomendation: {sample_response} <br>\"\n",
    "        f\"Data: {recomendation} <br> Recomendation:\"\n",
    "    )\n",
    "    \n",
    "#     print(f\"Rendered Prompt: {prompt}\")\n",
    "    return prompt\n",
    "\n",
    "prompt = render_prompt(example_request) # query with the same sample description from earlier\n",
    "\n",
    "prompt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 17. Query the LLM using the rendered prompt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "kwargs = {\n",
    "    \"num_beams\": 5, \n",
    "    \"no_repeat_ngram_size\": 3, \n",
    "    \"temperature\": 1.25, \n",
    "#     \"top_p\": .8,\n",
    "    \"top_k\": 147,\n",
    "    \"max_length\": 175,\n",
    "    \"early_stopping\": True,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "query_response = query(model_predictor, prompt, kwargs)\n",
    "generated_texts = parse_response(query_response)\n",
    "\n",
    "pprint(generated_texts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 18. Writing the wine recomender function\n",
    "\n",
    "Now we can wrap up the logic to query the vector store, render the prompt, query the model and present the recomendation back to the user."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recomend_wine(requested_description,\n",
    "                 num_beams=7,\n",
    "                 no_repeat_ngram_size=3,\n",
    "                 temperature=1.0,\n",
    "                 top_k=147,\n",
    "                 max_length=250,\n",
    "                 early_stopping=True,\n",
    "                 num_return_sequences=1,\n",
    "                 seed=0):\n",
    "    \n",
    "    prompt = render_prompt(requested_description) # query the vector store and render the prompt\n",
    "    query_response = query(model_predictor, prompt, kwargs) # query the llm with the prompt\n",
    "    generated_texts = parse_response(query_response) \n",
    "    \n",
    "    return generated_texts[0].split('<br>')[0]\n",
    "\n",
    "print(f\"Got request for: '{user_request}'\")\n",
    "print()\n",
    "recomendation_data = query_wines(user_request)\n",
    "print()\n",
    "print(f\"Retrieved recomendation from OpenSearch\")\n",
    "pprint(recomendation_data)\n",
    "print()\n",
    "print(\"Rendered Prompt:\")\n",
    "print(render_prompt(user_request))\n",
    "print()\n",
    "recommendation = recomend_wine('dessert wine. pairs with chocolate')\n",
    "print(f\"Recomendation: \\033[1m{recommendation}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17725398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
