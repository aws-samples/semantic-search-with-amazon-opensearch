{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "We will use the semantic search to provide the best matching wine based on the review description. [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) is a process that combines retrieval-based models and generative models to enhance natural language generation by retrieving relevant information and incorporating it into the generation process. In this notebook, we'll walk through enhancing an OpenSearch cluster search with generative AI to output conversational wine recommendations based on a desired description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "### 1. Install OpenSearch ML Python library\n",
    "\n",
    "For this notebook we require the use of a few key libraries. We'll use the Python clients for OpenSearch and SageMaker, and Python frameworks for text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensearch-py-ml accelerate tqdm --quiet\n",
    "!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31703e3d",
   "metadata": {},
   "source": [
    "### 2. Check PyTorch Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac12126",
   "metadata": {},
   "source": [
    "As in the previous modules, let's import PyTorch and confirm that the latest version of PyTorch is running. The version should already be 1.13.1 or higher. If not, please run the lab in order to get everything set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b532987",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c00375",
   "metadata": {},
   "source": [
    "Now we need to restart the kernel by running below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94df946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "### 3. Import libraries\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "### 4. Prepare data\n",
    "\n",
    "This lab combines semantic search with a generative model to present the retrieved data to the user . Below is a dataset of wine reviews, we'll sample this data set to recommend wines that resemble the user provided description.\n",
    "\n",
    "### Note\n",
    "You can download the dataset from various sources. One is Kaggle.\n",
    "https://www.kaggle.com/datasets/christopheiv/winemagdata130k?select=winemag-data-130k-v2.json\n",
    "\n",
    "After downloading and copying here, unzip in the working directory if it hasn't already been unzipped. Execute the following cells to inspect the dataset, transform it into a pandas DataFrame, and sample a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fca957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -o winemag-data-130k-v2.json.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('winemag-data-130k-v2.json')\n",
    "\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cf47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wm_list = df.sample(300,\n",
    "                   random_state=37).to_dict('records') # sample to keep lab quick\n",
    "\n",
    "wm_list[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "### 5. Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "Note: if you're using a region other than us-east-1, please update the region in the code below.\n",
    "\n",
    "#### Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = 'us-east-1' \n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kms = boto3.client('secretsmanager')\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "#credentials = boto3.Session().get_credentials()\n",
    "#auth = AWSV4SignerAuth(credentials, region)\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c1155",
   "metadata": {},
   "source": [
    "### 6. Get SageMaker endpoint for embedding\n",
    "\n",
    "---\n",
    "This is SageMaker Endpoint with GPT-J 6B parameters model to convert text into vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_endpoint_name=outputs['EmbeddingEndpointName']\n",
    "print(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc075ce",
   "metadata": {},
   "source": [
    "Define function to convert text into vector with SageMaker Embedding endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd689c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    #print(response)\n",
    "    response_json = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    embeddings = response_json[\"embedding\"]\n",
    "    if len(embeddings) == 1:\n",
    "        return [embeddings[0]]\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d66036",
   "metadata": {},
   "source": [
    "### 7. Test the embeddings endpoint with a sample phrase\n",
    "Using any text phrase, the endpoint converts the text to a vectorized array of size 768. We're also creating a function `embed_phrase` so that we can call it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_phrase(input_data):\n",
    "    input_str = json.dumps({\"text_inputs\": input_data})\n",
    "    encoded_input_str = input_str.encode(\"utf-8\")\n",
    "    features = query_endpoint_with_json_payload(encoded_input_str,embedding_endpoint_name)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bd3b3",
   "metadata": {},
   "source": [
    "Ask one question about wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_on_wine=\"A wine that pairs well with meat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e70458",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = embed_phrase(question_on_wine)\n",
    "\n",
    "print(len(result[0]))\n",
    "result[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "### 8. Create a index in Amazon Opensearch Service \n",
    "Whereas we previously created an index with 2-3 fields, this time we'll define the index with multiple fields: the vectorization of the `description` field, and all others present within the dataset.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we initiated ealier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"description_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 4096,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"description\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"designation\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"variety\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"country\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"winery\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"points\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84514d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch. Running this cell will recreate the index if you have already executed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index_name = \"wine_knowledge_base\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "### 9. Load the raw data into the Index\n",
    "Next, let's load the wine review data into the index we've just created. During ingest data defined by the `os_import` function, `description` field will also be converted to vector (embedding) by calling the previously created endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55e6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def os_import(record, aos_client, index_name):\n",
    "    description = record[\"description\"]\n",
    "    search_vector = embed_phrase(description)\n",
    "    aos_client.index(index=index_name,\n",
    "             body={\"description_vector\": search_vector[0], \n",
    "                   \"description\": record[\"description\"],\n",
    "                   \"points\":record[\"points\"],\n",
    "                   \"variety\":record[\"variety\"],\n",
    "                   \"country\":record[\"country\"],\n",
    "                   \"designation\":record[\"designation\"],\n",
    "                   \"winery\":record[\"winery\"]\n",
    "                  }\n",
    "            )\n",
    "\n",
    "print(\"Loading records...\")\n",
    "for record in tqdm(wm_list): \n",
    "    os_import(record, aos_client, index_name)\n",
    "print(\"Records loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have 300 hits in the index, or however many was specified earlier in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "### 10. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a wine whose review most closely matches the requested description. `retrieve_opensearch_with_semantic_search` embeds the search phrase, searches the index for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_opensearch_with_semantic_search(phrase, n=1):\n",
    "    search_vector = embed_phrase(phrase)[0]\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"description_vector\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"description_vector\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"description\",\"winery\",\"points\", \"designation\", \"country\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits'][0]\n",
    "    \n",
    "    result = {\n",
    "        \"description\":top_result['_source']['description'],\n",
    "        \"winery\":top_result['_source']['winery'],\n",
    "        \"points\":top_result['_source']['points'],\n",
    "        \"designation\":top_result['_source']['designation'],\n",
    "        \"country\":top_result['_source']['country'],\n",
    "        \"variety\":top_result['_source']['variety'],\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6820a",
   "metadata": {},
   "source": [
    "Use the semantic search to get similar records with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529077",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_request = retrieve_opensearch_with_semantic_search(question_on_wine)\n",
    "print(example_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98987279",
   "metadata": {},
   "source": [
    "### 11. Get SageMaker endpoint for content generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6343640",
   "metadata": {},
   "source": [
    "We are using Falcon 7B LLM in this lab. Please refere HuggingFace documentaion for more information: https://huggingface.co/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe808cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint_name=outputs['LLMEndpointName']\n",
    "print(llm_endpoint_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd47ade",
   "metadata": {},
   "source": [
    "Define function to use LLM to generate content. As LLM is trained with static, outdated data, and it does not have business domain knowledge, the generated content is not factual(hallucination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    model_predictions = json.loads(response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]\n",
    "\n",
    "def query_llm_with_hallucination(question):\n",
    "    payload = {\n",
    "        \"inputs\": question,\n",
    "        \"parameters\":{\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"num_return_sequences\": 1,\n",
    "            \"top_k\": 100,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": False,\n",
    "            \"return_full_text\": True,\n",
    "            \"temperature\": 0.9\n",
    "        }\n",
    "    }\n",
    "    query_response = query_llm_endpoint_with_json_payload(json.dumps(payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name)\n",
    "    return query_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb63ed8",
   "metadata": {},
   "source": [
    "Check the generated result from LLM with hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_texts = query_llm_with_hallucination(question_on_wine)\n",
    "\n",
    "print(f\"The recommened wine from LLM with hallucination: \\n{generated_texts[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e654",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation\n",
    "---\n",
    "To resolve LLM hallunination problem, we can more context to LLM so that LLM can use context information to fine the model and generated factual result. RAG is one of the solution to the LLM hallucination. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5564",
   "metadata": {},
   "source": [
    "### 12. Create a prompt for the LLM using the search results from OpenSearch\n",
    "\n",
    "We will be using the Falcon-7B model for one-shot generation, using a canned recommendation and response to guide the output. \n",
    "\n",
    "Before querying the model, the below function `generate_prompt_to_llm` is used to easily make a prompt for one-shot generation. The function takes in an input string to search the OpenSearch cluster for a matching wine, then compose the prompt to LLM. The prompt is in the following format:\n",
    "\n",
    "```\n",
    "A sommelier uses their vast knowledge of wine to make great recommendations people will enjoy. As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the following phrase: {original_question_on_win}.\n",
    "\n",
    "Data:{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\n",
    "\n",
    "Recommendation:I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\n",
    "\n",
    "Data: {retrieved_documents}\n",
    "\n",
    "Recommendation:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_to_llm(original_question_on_win):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(original_question_on_win)\n",
    "    print(\"retrieved relevant wine per your query is : \\n\" + str(retrieved_documents))\n",
    "    print(\"------------\")\n",
    "    one_shot_description_example = \"{'description': 'This perfumey white dances in intense and creamy layers of stone fruit and vanilla, remaining vibrant and balanced from start to finish. The generous fruit is grown in the relatively cooler Oak Knoll section of the Napa Valley. This should develop further over time and in the glass.', 'winery': 'Darioush', 'points': 92, 'designation': None, 'country': 'US'}\"\n",
    "    one_shot_response_example = \"I have a wonderful wine for you. It's a dry, medium bodied white wine from Darioush winery in the Oak Knoll section of Napa Valley, US. It has flavors of vanilla and oak. It scored 92 points in wine spectator.\"\n",
    "    prompt = (\n",
    "        f\"A sommelier uses their vast knowledge of wine to make great recommendations people will enjoy. As a sommelier, you must include the wine variety, the country of origin, and a colorful description relating to the following phrase: {original_question_on_win}.\\n\"\n",
    "        f\"Data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Data: {retrieved_documents} \\n Recommendation:\"\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26aa75c",
   "metadata": {},
   "source": [
    "### 13. Format LLM prompt and query using the generated prompt\n",
    "We also need a few more helper functions to query the LLM. `generate_llm_input` transforms the generated prompt into the correct input format, `render_llm_output` parses the LLM output. \n",
    "\n",
    "`query_llm_with_rag` combines everything we've done in this module. It does all of the following:\n",
    "- generate vector for the input\n",
    "- searches the OpenSearch index with semantic search for the relevant wine with \"description vector\"\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_input(data, **kwargs):\n",
    "    default_kwargs = {\n",
    "        \"num_beams\": 5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.01,\n",
    "        \"watermark\": True,\n",
    "        \"top_k\": 200,\n",
    "        \"max_length\": 200,\n",
    "        \"early_stopping\": True\n",
    "    }\n",
    "    \n",
    "    default_kwargs = {**default_kwargs, **kwargs}\n",
    "    \n",
    "    input_data = {\n",
    "        \"inputs\": data,\n",
    "        \"parameters\": default_kwargs\n",
    "    }\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def query_llm_with_rag(description, **kwargs):\n",
    "    prompt = generate_prompt_to_llm(description)\n",
    "    query_payload = generate_llm_input(prompt, **kwargs)\n",
    "    response = query_llm_endpoint_with_json_payload(json.dumps(query_payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de7273",
   "metadata": {},
   "source": [
    "#### And finally, let's call the function and get a wine recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = query_llm_with_rag(\"A wine that pairs well with meat.\")\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adc9a2",
   "metadata": {},
   "source": [
    "### Additional info: changing kwargs for querying the LLM\n",
    "If you want to change or add new parameters for LLM querying, you're able to add in new keyword arguments to the `query_llm` function. For example, to change the `temperature` value, simply change the function call:\n",
    "`query_llm(description phrase, temperature = new float value)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
