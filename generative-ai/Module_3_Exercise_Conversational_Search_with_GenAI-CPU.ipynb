{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d41ec6",
   "metadata": {},
   "source": [
    "# Exercise - Build Your Own Conversational Search with GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd860b-5997-45a6-869b-06793935c75d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this lab, you will build your own Generative AI application with Conversational Search powered by Large Language Models(LLMs) by leveraging the Langchain framework to implement Retrieval Augmented Generation(RAG) solution with OpenSearch Vector DB. \n",
    "\n",
    "You will be provided with skeleton code blocks that can be completed as per your specific use-case and requirements. Feel free to refer to the previous modules in this workshop to fill in the `**TODO**` sections of the code blocks below to build you own custom conversational search application.\n",
    "\n",
    "We will also explore ways to improve RAG systems through\n",
    "\n",
    "For more information about LangChain RAG, please refer: https://python.langchain.com/docs/use_cases/question_answering/\n",
    "\n",
    "---\n",
    "\n",
    "The lab includes the following steps:\n",
    "1. [Step 1: Initialize](#Step-1:-Initialize)\n",
    "2. [Step 2: Select SageMaker or Bedrock used for embedding and content generation](#Step-2:-Select-SageMaker-or-Bedrock-used-for-embedding-and-content-generation)\n",
    "3. `TODO:` [Step 3: Load documents into OpenSearch's Vector DB](#Step-3:-Load-documents-into-OpenSearch's-vector-database)\n",
    "4. `TODO:`[Step 4: Retrieval Augmented Generation](#TODO-Step-4:-Retrieval-Augmented-Generation)\n",
    "5. `TODO:`[Step 5: Conversational search by memorizing the history](#TODO-Step-5:-Conversational-search-by-memorizing-the-history)\n",
    "\n",
    "\n",
    "To be completed in this lab:\n",
    "- [&#x2611;] embedding and store into OpenSearch\n",
    "- [&#x2611;] OpenSearch ANN engine and number of neighbors of graph\n",
    "- [&#x2611;] Choose different approach to use retrieved documents as context: #stuff, refine, map_reduce, and map_rerank\n",
    "- [&#x2612;] Number of conversation history\n",
    "- [&#x2612;] RAG Improvement patterns\n",
    "    - Base Prompt\n",
    "    - Chucking Approach\n",
    "    - Query Transformations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03874004-bd68-4fac-a05a-3834986a8e89",
   "metadata": {},
   "source": [
    "## Step 1: Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34490d00-970a-49ac-97f5-1999ff4ca03f",
   "metadata": {},
   "source": [
    "Install required library such as OpenSearch client library, LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06184986",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker==2.186.0 --quiet\n",
    "%pip install opensearch-py==2.3.1 --quiet\n",
    "%pip install wikipedia unstructured transformers==4.33.2 --quiet\n",
    "%pip install langchain==0.0.293 --quiet\n",
    "%pip install --upgrade boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e422af0-3961-4e99-9f90-f46d3cf78f0b",
   "metadata": {},
   "source": [
    "Initialize SageMaker, Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d0256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad970f25-32cf-4bd8-bfa4-f13e361a5b20",
   "metadata": {},
   "source": [
    "### Get Cloud Formation stack output variables\n",
    "\n",
    "We also need to grab some key values from the infrastructure we provisioned using CloudFormation. To do this, we will list the outputs from the stack and store this in \"outputs\" to be used later.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635d4ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "region = aws_region\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "kms = boto3.client('secretsmanager')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"semantic-search\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OpenSearchDomainEndpoint']\n",
    "aos_credentials = json.loads(kms.get_secret_value(SecretId=outputs['OpenSearchSecret'])['SecretString'])\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d926ca92-3373-47a7-973e-3136ebfa2370",
   "metadata": {},
   "source": [
    "#### **Note**: To verify deployed endpoint for embedding and content generation model please refer to Step:2 in Module 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49640d",
   "metadata": {},
   "source": [
    "### Get endpoint for embedding\n",
    "\n",
    "---\n",
    "This is SageMaker Endpoint with GPT-J 6B parameters model to convert text into vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5962e3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_endpoint_name=outputs['EmbeddingEndpointName']\n",
    "print(embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b48027",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get endpoint for content generation\n",
    "\n",
    "We use Falcon large language model to generate text. Our Falcon model has 7 billion parameters. \n",
    "It is a smallest Falcon model available and provides a good balance between accuracy and hardware costs to run the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecca035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm_endpoint_name=outputs['LLMEndpointName']\n",
    "print(llm_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31632621",
   "metadata": {},
   "source": [
    "### Setup Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83499f9a",
   "metadata": {},
   "source": [
    "You need to change `os.environ[\"BEDROCK_ENDPOINT_URL\"]` per your Bedrock version(production or developer access). Please refer to the documentation for the right endpoint url based on the region -[Amazon Bedrock Endpoints](https://docs.aws.amazon.com/bedrock/latest/userguide/endpointsTable.html)\n",
    "\n",
    "\n",
    "Amazon Bedrock users need to **request access** to models before they are available for use. If you want to add additional models for text, chat, and image generation, you need to request access to models in Amazon Bedrock. To request access to additional models, \n",
    "\n",
    "1. Navigate to: [Amazon Bedrock Console](https://console.aws.amazon.com/bedrock) and;\n",
    "2. Select the **Model access** link in the left side navigation panel in the Amazon Bedrock console.\n",
    "3. Select the check box next to the model you want to add access to and **Save Changes**\n",
    "\n",
    "If Bedrock is available in your account, set `is_bedrock_available` to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_bedrock_available=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da798856",
   "metadata": {},
   "source": [
    "Here we use Claude2 text generation model. Same question as before and see if there is any hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffeedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "claude_llm_hallucination = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "claude_llm_hallucination.model_kwargs = {'temperature': 0.9, \"max_tokens_to_sample\": 1024}\n",
    "\n",
    "if is_bedrock_available:\n",
    "    claude_result = claude_llm_hallucination(question)\n",
    "    print(claude_result)\n",
    "else:\n",
    "    print(\"Bedrock is unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e258274",
   "metadata": {},
   "source": [
    "## Step 2: Select SageMaker or Bedrock used for embedding and content generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f219795",
   "metadata": {},
   "source": [
    "Select one the the LLM used in the lab.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "\n",
    "llm_selection = [\n",
    "    \"SageMaker\",\n",
    "    \"Bedrock\",\n",
    "]\n",
    "\n",
    "llm_dropdown = Dropdown(\n",
    "    options=llm_selection,\n",
    "    value=\"SageMaker\",\n",
    "    description=\"Select a LLM\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(llm_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd2206",
   "metadata": {},
   "source": [
    "#### Note: If Bedrock is unavailable, we have to use SageMaker as backup plan.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276082d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_category = llm_dropdown.value\n",
    "\n",
    "if not is_bedrock_available:\n",
    "    llm_category = \"SageMaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"You selected {0} as LLM\".format(llm_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc628cac",
   "metadata": {},
   "source": [
    "#### Content Handler util for defining LLM with SagemakerEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea71507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import Dict\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.memory import DynamoDBChatMessageHistory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "    \n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        #print(\"Prompt Input:\\n\" + input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(\"LLM generated text:\\n\" + response_json[0][\"generated_text\"])\n",
    "        return response_json[0][\"generated_text\"]\n",
    "    \n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdeaa9",
   "metadata": {},
   "source": [
    "## `TODO:` Step 3: Load documents into OpenSearch's vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128529b-29fc-4e00-9faa-e5de2de70023",
   "metadata": {},
   "source": [
    "Langchain provides various document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading unstructured word documents.\n",
    "\n",
    "---\n",
    "\n",
    "The following is data flow diagram of loading documents and store vector into OpenSearch.\n",
    "\n",
    "![retriever](../image/module3/document_loader)\n",
    "\n",
    "\n",
    "Document loaders expose a \"load\" method for loading data as documents from a configured source. Here, we use `UnstructuredURLLoader` to load OpenSearch best practice web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae050e",
   "metadata": {},
   "source": [
    "You could use multiple document loaders provided by langchain based on the type of source documents. Refer [Langchain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders) for more details on completing the next section of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO : Use Langchain documetn loaders to load source data into the vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57321a3a-db5c-4589-b6a4-6a007154d067",
   "metadata": {},
   "source": [
    "Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with Amazon Opensearch Service domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede13e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "\n",
    "#auth = (\"master\",\"Semantic123!\")\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a03874-5a51-456e-b36c-e31771d73ad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LangChain embedding endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671fdfd-0632-49b8-b7f7-05493baa3ece",
   "metadata": {},
   "source": [
    "To build a simiplied QA application with LangChain, we need to wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. That requires a overwrite methods of `SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding mdoel.\n",
    "\n",
    "---\n",
    "\n",
    "Embedding language model is GPT-J, and the endpoint name is `embedding_endpoint_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dd4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Callable\n",
    "import json\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.schema import Document\n",
    "\n",
    "class BulkSagemakerEndpointEmbeddings(SagemakerEndpointEmbeddings):\n",
    "        def embed_documents(\n",
    "            self, texts: List[str], chunk_size: int = 5\n",
    "        ) -> List[List[float]]:\n",
    "            results = []\n",
    "            _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "\n",
    "            for i in range(0, len(texts), _chunk_size):\n",
    "                response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "                results.extend(response)\n",
    "            return results\n",
    "        \n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "            input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "            return input_str.encode('utf-8') \n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            embeddings = response_json[\"embedding\"]\n",
    "            if len(embeddings) == 1:\n",
    "                return [embeddings[0]]\n",
    "            return embeddings\n",
    "\n",
    "print(embedding_endpoint_name)\n",
    "sagemaker_embeddings = BulkSagemakerEndpointEmbeddings( \n",
    "            endpoint_name=embedding_endpoint_name,\n",
    "            region_name=aws_region, \n",
    "            content_handler=EmbeddingContentHandler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56027a",
   "metadata": {},
   "source": [
    "### Bedrock embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b84fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id='amazon.titan-embed-text-v1',client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57297dfc-3816-4039-8a61-35e0ff1965cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OpenSearch vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2bf2a9",
   "metadata": {},
   "source": [
    "### Provide embedding service based on selection between SageMaker and Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49401f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        embeddings = sagemaker_embeddings\n",
    "    case \"Bedrock\":\n",
    "        embeddings = bedrock_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678f2f9",
   "metadata": {},
   "source": [
    "### TODO: Ingest the documents into OpenSearch Vector store using [OpenSearchVectorSearch](https://python.langchain.com/docs/integrations/vectorstores/opensearch) provided by langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0aeb8-d5e5-4f1c-b9c4-432afdbf93b4",
   "metadata": {},
   "source": [
    "Use `OpenSearchVectorSearch` in LangChain to ingest vector into OpenSearch. You can specify more parameters to create kNN index with specified properties. Some parameters like:\n",
    "\n",
    "- `engine`: “nmslib”, “faiss”, “lucene”; `default`: “nmslib”\n",
    "\n",
    "- `space_type`: “l2”, “l1”, “cosinesimil”, “linf”, “innerproduct”; `default`: “l2”\n",
    "\n",
    "- `ef_search`: Size of the dynamic list used during k-NN searches. Higher values lead to more accurate but slower searches; `default`: 512\n",
    "\n",
    "- `ef_construction`: Size of the dynamic list used during k-NN graph creation. Higher values lead to more accurate graph but slower indexing speed; `default`: 512\n",
    "\n",
    "- `m`: Number of bidirectional links created for each new element. Large impact on memory consumption. Between 2 and 100; `default`: 16\n",
    "\n",
    "\n",
    "**Note**: When you use LangChain `OpenSearchVectorSearch` to store embedding with OpenSearch kNN index, you can specify parameters to choose different Approximate Near Neighbour(ANN) algorithms. For more information, please refer OpenSearch kNN documentaion: https://opensearch.org/docs/latest/search-plugins/knn/knn-index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d425b-0aa0-4e40-9be8-db082b3c5e0c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "os_domain_ep = 'https://'+aos_host\n",
    "\n",
    "embedding_index_name = #<name of the OS index>\n",
    "\n",
    "# TODO: Code to ingest data into OS index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369f552-2b2c-49fe-955b-fd814183b720",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check whether the OS index is created successfully\n",
    "aos_client.indices.get(index=embedding_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0e71e-bf17-4f6e-aaff-be56a874c212",
   "metadata": {},
   "source": [
    "## `TODO` Step 4: Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db375107-9d31-4e35-acda-a5d20941ca7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To mitigate LLM hallucination, we can provide some context to LLM and let LLM generated answer with the context. The following diagram show RAG data flow:\n",
    "\n",
    "![rag](../image/module3/workflow)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2657b1",
   "metadata": {},
   "source": [
    "Define SageMaker LLM endpoint\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29542ec7-0296-4f3e-b89d-fe9064665960",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_params = {\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 200,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0001\n",
    "        }\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e45446",
   "metadata": {},
   "source": [
    "Define Bedrock content generation LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_params = {\n",
    "    'temperature': 0.00001,\n",
    "    \"max_tokens_to_sample\": 1024\n",
    "    }\n",
    "\n",
    "bedrock_claude_llm = Bedrock(model_id=\"anthropic.claude-instant-v1\", client=boto3_bedrock)\n",
    "bedrock_claude_llm.model_kwargs = bedrock_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7422be1",
   "metadata": {},
   "source": [
    "### Provide content geeration service based on selection between SageMaker and Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22732f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        llm = sagemaker_llm\n",
    "    case \"Bedrock\":\n",
    "        llm = bedrock_claude_llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9bf32",
   "metadata": {},
   "source": [
    "Define `RetrievalQA` Chain with SageMaker or Bedrock LLM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd71470",
   "metadata": {},
   "source": [
    "### TODO: Use the OpenSearch vector store as retriever to get similiar documents with query. \n",
    "\n",
    "We can also specify similarity scrore threshhold to return high relevant documents. Use \"k\" to limit how many documents to be returned. Refer to [VectorStoreRetriever](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.html#langchain.vectorstores.opensearch_vector_search.OpenSearchVectorSearch.as_retriever) for reference.\n",
    "\n",
    "Category of chains are used for interacting with indexes. The purpose these chains is to combine your own data (stored in the indexes) with LLMs. The best example of this is question answering over your own documents\n",
    "\n",
    "1. `stuff`: The stuff documents chain (\"stuff\" as in \"to stuff\" or \"to fill\") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM. This chain is well-suited for applications where documents are small and only a few are passed in for most calls.\n",
    "\n",
    "2. `refine`: The Refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer. Since the Refine chain only passes a single document to the LLM at a time, it is well-suited for tasks that require analyzing more documents than can fit in the model's context.\n",
    "\n",
    "3. `map reduce`: The map reduce documents chain first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combine documents chain to get a single output (the Reduce step).\n",
    "\n",
    "4. `re-rank`: The map re-rank documents chain runs an initial prompt on each document, that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest scoring response is returned.\n",
    "\n",
    "The `chain_type` choice can certainly help in improving your RAG system. Select the chain_type for your use-case and define a `RetrievalQA.from_chain_type()`\n",
    "\n",
    "Learn more about [chain_types](https://python.langchain.com/docs/modules/chains/document/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a1e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO \n",
    "# 1. Define sagemaker and Bedrock Retriever\n",
    "# 2. Define the qa chain_type using the retriver and experiment with chain_type\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=#<> #stuff, refine, map_reduce, and map_rerank\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f869cfe8-963f-4053-aa00-22f7ff76a132",
   "metadata": {},
   "source": [
    "Use RAG to generate answer to the same question before. Compare the content generated with RAG and LLM without context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feaca4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Question is:\" + question)\n",
    "\n",
    "#langchain.debug=False\n",
    "result = qa({\"query\": question})\n",
    "\n",
    "print(\"result:\" + result[\"result\"])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c88750-e300-45f9-988f-3e238e72f84f",
   "metadata": {},
   "source": [
    "## `TODO` Step 5: Conversational search by memorizing the history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d64dc6-df81-4df2-806a-14cebec9b6ce",
   "metadata": {},
   "source": [
    "### LangChain Memory with Amazon DynamoDB as data store\n",
    "\n",
    "In the above example, you can ask any questions to the system. However there is no relation among the questions. In a typical search system, you may want to implement conversational search. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. LangChain provides a lot of utilities for adding memory to a system. These utilities can be used by themselves or incorporated seamlessly into a chain. In this lab, we use [Amazon DynamoDB](https://aws.amazon.com/dynamodb/) as data store of history message.\n",
    "\n",
    "---\n",
    "The data flow of conversational search with memory is as following:\n",
    "\n",
    "![rag](../image/module8/rag-with-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3824f779-5233-4047-a26a-38e035354445",
   "metadata": {},
   "source": [
    "---\n",
    "Here we create new session and use DynamoDB as backend to store history conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea68c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddb_table_name = \"conversation-history-memory\"\n",
    "session_id = str(uuid4())\n",
    "chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "\n",
    "messages = chat_memory.messages\n",
    "\n",
    "# Maintains immutable sessions\n",
    "# If previous session was present, create\n",
    "# a new session and copy messages, and \n",
    "# generate a new session_id \n",
    "if messages:\n",
    "    session_id = str(uuid4())\n",
    "    chat_memory = DynamoDBChatMessageHistory(\n",
    "        table_name=ddb_table_name,\n",
    "        session_id=session_id\n",
    "    )\n",
    "    # This is a workaround at the moment. Ideally, this should\n",
    "    # be added to the DynamoDBChatMessageHistory class\n",
    "    try:\n",
    "        messages = messages_to_dict(messages)\n",
    "        chat_memory.table.put_item(\n",
    "            Item={\"SessionId\": session_id, \"History\": messages}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f025891e",
   "metadata": {},
   "source": [
    "### TODO: Define memory store to store conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Define memory store\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dbb51f-2460-4a8e-a482-08be21b02e26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Create `ConversationalRetrievalChain` to combines the chat history and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question-answering chain to return a response.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb0a3e-40a1-4927-8833-5f185410eef8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "params = {\n",
    "        \"max_length\": 2048,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"top_k\": 200,\n",
    "        \"top_p\": 0.9,\n",
    "        \"do_sample\": False,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0001\n",
    "        }\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        region_name=aws_region,\n",
    "        model_kwargs=params,\n",
    "        content_handler=content_handler,\n",
    ")\n",
    "\n",
    "condense_template = \"\"\"system: generate one standalone question.\n",
    "Given the following conversation between <chat-history> and </chat-history> \n",
    "and follow up question between <follow-up-question> and </follow-up-question>, \n",
    "rephrase the follow up question to be a standalone question in its original language. \n",
    "The standalone question will only contains one sentence and it must end with '?'\n",
    "\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-question>\n",
    "{question}\n",
    "</follow-up-question>\n",
    "\n",
    "standalone question:\n",
    "\"\"\"\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(condense_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c692cc3b",
   "metadata": {},
   "source": [
    "### TODO: Define prompt used by LLM to generate answers with context information and original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9221939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Define prompt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "match llm_category:\n",
    "    case \"SageMaker\":\n",
    "        llm = sagemaker_llm\n",
    "    case \"Bedrock\":\n",
    "        llm = bedrock_claude_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bc823",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_memory = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    condense_question_prompt=CONDENSE_QUESTION_PROMPT,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt_template2},\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd000156-1110-4429-b8b9-1a34f1e5ba61",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For the first question, there is no history. It is just standard RAG process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc1671-79f2-4056-8248-6eff1e3c16fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = qa_with_memory(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02d8f4-4a23-412f-a76d-41e75037ad9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(\"result:\" + str(result))\n",
    "print(\"\\nAnswer:\\n\" + str(result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cdd8c-a04d-41fe-99e6-33b531015260",
   "metadata": {},
   "source": [
    "### Second question\n",
    "Try to ask one more question, `ConversationalRetrievalChain` will use the first question, first question's answer and second question as prompt to LLM to generate new question. The prompt to LLM is like following:\n",
    "\n",
    "```python\n",
    "\n",
    "_template = \"\"\"system: generate one standalone question.\n",
    "Given the following conversation between <chat-history> and </chat-history> \n",
    "and follow up question between <follow-up-question> and </follow-up-question>, \n",
    "rephrase the follow up question to be a standalone question in its original language. \n",
    "The standalone question will only contains one sentence and it must end with '?'\n",
    "\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-question>\n",
    "{question}\n",
    "</follow-up-question>\n",
    "\n",
    "standalone question:\n",
    "```\n",
    "\n",
    "After get the new question from LLM, it will search relevant document from OpenSearch vector store and get relevant documents, then combine the new question and relevant documents as prompt to go through RAG process. The prompt to LLM is like following:\n",
    "\n",
    "```python\n",
    "prompt_template = \"\"\"Answer the question as truthfully as possible by using the provided informaiton in >>CONTEXT<<. If the answer is not contained within the >>CONTEXT<<, respond with \"I can't answer that\".\n",
    "\n",
    ">>CONTEXT<<:\n",
    "{context}\n",
    "\n",
    ">>QUESTION<<:\n",
    "{question}\n",
    "\n",
    ">>Answer<<:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "In summary, `ConversationalRetrievalChain` will call LLM twice:\n",
    "1. Use history question, history answer and latest question as prompt to generate new question\n",
    "2. Use new question generated in the first step, query relevant documents. Combine relevant documents and new question as prompt to LLM to generate answer.\n",
    "\n",
    "You can also see the verbose message like following:\n",
    "\n",
    "---\n",
    "\n",
    "### First call to LLM:\n",
    "\n",
    "![generate new question](../image/module8/conversation-new-question.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Second call to LLM:\n",
    "\n",
    "![generate final answer](../image/module8/conversation-final-answer.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e63fc-6c54-479b-b7c5-8310a16896a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_following_question = 'if my data growth is very fast'\n",
    "second_result = qa_with_memory(second_following_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a8099a-a3be-4996-9852-97a6282d85e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"second answer:\" + str(second_result[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8786b9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
